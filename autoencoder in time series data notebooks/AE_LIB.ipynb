{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1488fa6",
   "metadata": {},
   "source": [
    "# LIBS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "614110dd",
   "metadata": {},
   "source": [
    "!pip install import_ipynb\n",
    "!pip install awswrangler\n",
    "!pip install tensorflow\n",
    "#!pip install tensorflow==2.11.0 \n",
    "!pip install keras\n",
    "!pip install fastparquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2821951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('datalobsterplot.mplstyle')\n",
    "plt.rcParams[\"figure.figsize\"] = (20,5)\n",
    "import awswrangler as wr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ea68a2d",
   "metadata": {},
   "source": [
    "## COSMETICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64ef2a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURE PANDAS TO DISPLAY FULL TEXT IN CELLS\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c572f8e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#notebook { padding-top:0px !important; } .container { width:100% !important; } .end_space { min-height:0px !important; } </style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# GET RID OF GRAY SPACES\n",
    "\n",
    "from IPython.display import display, HTML, clear_output\n",
    "display(HTML(\n",
    "    '<style>'\n",
    "        '#notebook { padding-top:0px !important; } ' \n",
    "        '.container { width:100% !important; } '\n",
    "        '.end_space { min-height:0px !important; } '\n",
    "    '</style>'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd363a2d",
   "metadata": {},
   "source": [
    "## ML LIBs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "077ae46b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-12 14:59:05.895250: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-09-12 14:59:06.690590: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Input, Conv1D, MaxPooling1D, UpSampling1D\n",
    "from keras.layers import LSTM, Input, Dropout\n",
    "from keras.layers import RepeatVector\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.regularizers import l1, l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c8af474a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9256de76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1ebb4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e5e4699",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save Model\n",
    "import tempfile\n",
    "import boto3 as bt\n",
    "import joblib\n",
    "#Save plot\n",
    "import io\n",
    "#\n",
    "from tqdm import tqdm\n",
    "#\n",
    "from IPython.display import clear_output\n",
    "#\n",
    "from statistics import mean\n",
    "#\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7889eeb0",
   "metadata": {},
   "source": [
    "# READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2b90532b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GET THE DATA USING YEAR MONTH DAY - GET CERTAIN DAYS OF 1 MONTH (DEPRECATED!!)\n",
    "def read_days_files(path,year,month,days,time_column,file_format=\".parquet\",use_columns=[\"tm\",\"m_rms\",\"a_rms\",\"l\"]):\n",
    "    dfs=[]\n",
    "    for day in days:\n",
    "        file_paths = wr.s3.list_objects(path.format(year,month,day))\n",
    "        if len(file_paths)>0:\n",
    "            for file_path in file_paths:\n",
    "                df=pd.read_parquet(file_path,columns=use_columns)\n",
    "                dfs.append(df)\n",
    "    if len(dfs)>0:\n",
    "        df= pd.concat(dfs)\n",
    "        df.sort_values(time_column,inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "28ac65c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA PARQUETS \n",
    "\n",
    "\n",
    "def read_dates_files(path,dates,time_column,file_format=\".parquet\",use_columns=[\"tm\",\"m_rms\",\"a_rms\",\"l\"]):\n",
    "    \"\"\"\n",
    "    Reads days of parquet files and from a list of dates\n",
    "    Arguments:\n",
    "        path: path of the parquet files, name must be of format ending with /{}/{}/{}/ for the year month and day to be filled\n",
    "        dates: list of dates\n",
    "        time_column: which column of the parquet file will be transformed to a date time and for sorting\n",
    "        file_format: csv of pdf etc...\n",
    "        use_colums: which columns will be extracted from the parquet file\n",
    "    Returns:\n",
    "        A concatenated pandas dataframe that contains the selected columns and the selected dates. \n",
    "    \"\"\"\n",
    "    dfs=[]\n",
    "    for date in dates:\n",
    "        date=pd.to_datetime(date)\n",
    "        year=str(date.year).zfill(2)\n",
    "        month=str(date.month).zfill(2)\n",
    "        day=str(date.day).zfill(2)\n",
    "        \n",
    "        file_paths = wr.s3.list_objects(path.format(year,month,day))\n",
    "\n",
    "        if len(file_paths)>0:\n",
    "            for file_path in file_paths:\n",
    "                try:\n",
    "                    df=pd.read_parquet(file_path,columns=use_columns)\n",
    "                    dfs.append(df)\n",
    "                except Exception as E:\n",
    "                    print(\"Dosya okunamadı \"+\"_\".join([year,month,day]))\n",
    "                    print(E)\n",
    "     \n",
    "    if len(dfs)>0:\n",
    "        df= pd.concat(dfs)\n",
    "        df.sort_values(time_column,inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfeff569",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bee7447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATA BETWEEN START AND END (THIS ONE IS USED)\n",
    "\n",
    "def read_spesific_files(path,start_end_df,use_cols,time_column):\n",
    "    \"\"\"\n",
    "    Reads time intervals from parquet files, \n",
    "    Arguments:\n",
    "        path: path of the parquet files, name must be of format ending with /{}/{}/{}/ for the year month and day to be filled\n",
    "        start_end_df: pandas dataframe that must have start and end columns defining the desired intervals\n",
    "        use_cols: which columns will be extracted from the parquet file\n",
    "        time_column: which column of the parquet file will be transformed to a date time and for sorting\n",
    "\n",
    "    Returns:\n",
    "        A concatenated pandas dataframe that contains the selected columns and the selected time intervals. \n",
    "    \"\"\"\n",
    "    \n",
    "    starts=start_end_df.start\n",
    "    ends=start_end_df.end\n",
    "    \n",
    "    dfs=[]\n",
    "\n",
    "    for i in range(len(starts)):\n",
    "        dates=pd.date_range(starts.iloc[i].date(),ends.iloc[i].date())\n",
    "        df=read_dates_files(path,dates,time_column,file_format=\".parquet\",use_columns=use_cols)\n",
    "        if time_column in df.columns:\n",
    "            df=df.loc[(ends.iloc[i]>df[time_column])&(df[time_column]>starts.iloc[i])] # the interval\n",
    "            dfs.append(df)\n",
    "    \n",
    "    if len(dfs)>0:\n",
    "        df= pd.concat(dfs)\n",
    "        df.sort_values(time_column,inplace=True)\n",
    "        df.reset_index(drop=True,inplace=True)\n",
    "        return df\n",
    "    else:\n",
    "        return pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b32e02f",
   "metadata": {},
   "source": [
    "# General"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c6346fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# USED TO WRITE THE MODELS, \n",
    "\n",
    "def get_s3_Bucketname_and_Key(path):\n",
    "    \"\"\"\n",
    "    Split the path text and extracth bucket name, key and filename, \n",
    "    Arguments:\n",
    "        path: path of the parquet file\n",
    "\n",
    "    Returns:\n",
    "        bucket name, key and filename\n",
    "    \"\"\"\n",
    "    s3_bucket_name=path.split(\"/\")[2]\n",
    "    s3_key=\"/\".join(path.split(\"/\")[3:])\n",
    "    s3_file_name=\"/\".join(path.split(\"/\")[-1:])\n",
    "    return s3_bucket_name,s3_key,s3_file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "50a88488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_subasset_train_cols(cols,path,start_end_df,time_column):\n",
    "    \n",
    "    \"\"\"\n",
    "    Get the colum names of a specific path, \n",
    "    Arguments:\n",
    "        cols: the columns that we want to train\n",
    "        path: path of the s3 file /{}/{}/{}/\n",
    "        start_end_df: dataframe containing the start and end times\n",
    "        time_column: which column of the parquet file will be transformed to a date time and for sorting\n",
    "\n",
    "    Returns:\n",
    "        Returns a list of columns that exist among the ones entered as input to the function. \n",
    "        If a desired column does not exist in the data, it will be excluded from the original list. \n",
    "        Ideally, the input and output will be the same.\n",
    "        \n",
    "    \"\"\"\n",
    "    existing_train_cols=cols.copy()\n",
    "    for i in range(len(start_end_df)):\n",
    "        df=read_spesific_files(path,start_end_df.iloc[i:i+1],use_cols=None,time_column=time_column)\n",
    "        if len(df)==0:\n",
    "            continue\n",
    "        subasset_one_cycle_all_cols=df.columns\n",
    "        subasset_one_cycle_existing_cols=[col for col in cols if col in subasset_one_cycle_all_cols]\n",
    "        existing_train_cols=[col for col in subasset_one_cycle_existing_cols if col in existing_train_cols]\n",
    "    \n",
    "    return existing_train_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc2fdf5",
   "metadata": {},
   "source": [
    "#### Lınear Reconstructed Columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eacfb9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_shape(np_arr):\n",
    "    \"\"\"\n",
    "    Reshape the 3 dimensianl array to 2 dimensional array. The arrays are already 2 dimensional with the 3rd dimension haveing a single column  \n",
    "    Arguments:\n",
    "        np_arr: input array\n",
    "\n",
    "    Returns:\n",
    "        2 reshaped array. \n",
    "    \"\"\"\n",
    "    np_arr=np_arr.reshape(np_arr.shape[0],np_arr.shape[1])\n",
    "    return np_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3bfe51cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_array_to_signal_mean(X_,window_size=128):\n",
    "    \"\"\"\n",
    "    Create the reconstructed signal using the mean:\n",
    "        XXXXXXXX\n",
    "         XXXXXXXX\n",
    "           XXXXXXX\n",
    "            |\n",
    "            mean of this column of values is registered as the reconstructed signal at that location. \n",
    "            \n",
    "        For the beginning and the end, the mean is taken on a smaller number of signals, 1,2,3 ... untill windowsize\n",
    "    Arguments:\n",
    "        X_: List of reconstructed signal segments. Each element is a list of [windowsize]\n",
    "            [\n",
    "                [T1,T2,T3]\n",
    "                [T2,T3,T4]\n",
    "                [T3,T4,T5]\n",
    "                ...\n",
    "            \n",
    "            ]\n",
    "            T is for time\n",
    "        window_size: Size of the window\n",
    "\n",
    "    Returns:\n",
    "        One single list for the reconstruction\n",
    "    \"\"\"\n",
    "    array=np.zeros((len(X_)+(window_size-1)))\n",
    "    for i in range(len(X_)):\n",
    "        array[i:window_size+i]+=X_[i]\n",
    "    for i in range(len(array)):\n",
    "        if i < window_size:\n",
    "            array[i]/=(i+1)\n",
    "        elif i> len(array)-window_size:\n",
    "            array[i]/=len(array)-i\n",
    "        else:\n",
    "            array[i]/=window_size\n",
    "    \n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8e988c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstructed_array_to_signal_variance(X_,window_size=128):\n",
    "    \n",
    "    \"\"\"\n",
    "    Create the reconstructed signal using the variance:\n",
    "        XXXXXXXX\n",
    "         XXXXXXXX\n",
    "           XXXXXXX\n",
    "            |\n",
    "            variance of this column of values is registered as the reconstructed signal at that location. \n",
    "            \n",
    "        For the beginning and the end, the variance is taken on a smaller number of signals, 1,2,3 ... untill windowsize\n",
    "    Arguments:\n",
    "        X_: List of reconstructed signal segments. Each element is a list of [windowsize]\n",
    "            [\n",
    "                [T1,T2,T3]\n",
    "                [T2,T3,T4]\n",
    "                [T3,T4,T5]\n",
    "                ...\n",
    "            \n",
    "            ]\n",
    "            T is for time\n",
    "        window_size: Size of the window\n",
    "\n",
    "    Returns:\n",
    "        One single list for the containing the variance\n",
    "    \"\"\"\n",
    "    array=[[] for i in range(len(X_)+(window_size-1))]\n",
    "    \n",
    "    for i in range(len(X_)):\n",
    "        for j in range(window_size):\n",
    "            array[i+j].append(X_[i][j])\n",
    "    \n",
    "    for i in range(len(array)):\n",
    "        array[i]=np.var(array[i])\n",
    "    \n",
    "    for i in range(window_size-1):\n",
    "        array[i]=0\n",
    "        array[i*-1-1]=0\n",
    "    array=np.array(array)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af6b13b",
   "metadata": {},
   "source": [
    "#### VIZUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2cba2da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_cols(df,cols,time_column):   # DEPRECATED\n",
    "    \n",
    "    \"\"\"\n",
    "    Plot multiple columns with respect to time, \n",
    "    Arguments:\n",
    "        df: the dataframe\n",
    "        cols: the columns that we want to plot\n",
    "        time_column: which column of the df  will be transformed to a date time \n",
    "\n",
    "    Returns:\n",
    "       No return value, show plot.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    df.index=df[time_column]  \n",
    "    for col in cols:\n",
    "        df[col].plot(label=col)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    df.reset_index(inplace=True,drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2ab3694d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_twinx(data1,data2,timedata,title,data1_ylabel,data2_ylabel,is_save_plot,plot_vertical_line,save_bucket=None,save_key=None,save_format=\"pdf\",vertical_line_points=None):\n",
    "    \"\"\"\n",
    "    Create a plot with 2 y axes\n",
    "    Arguments:\n",
    "        data1: series ax.plot compatible for left side\n",
    "        data2: series ax.plot compatible for right side\n",
    "        timedata: x axis values for the plot\n",
    "        title: title of the graph\n",
    "        data1_ylabel: y label for the left hand data\n",
    "        data2_ylabel: y label for the right hand data\n",
    "        is_save_plot: boolena indicating if the plot should be saved\n",
    "        plot_vertical_line: add vertical lines or not\n",
    "        save_bucket: bucket where the output graph will be saved\n",
    "        save_key: key of the saved file\n",
    "        save_format=\"pdf\": format of the output file\n",
    "        vertical_line_points: list of vertical lines (x axis coordonnates)\n",
    "\n",
    "    Returns:\n",
    "       No return value, show plot and save (optional)\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    colors =plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    fig, ax1 = plt.subplots()\n",
    "    \n",
    "    color = colors[0]\n",
    "    ax1.bar(timedata, data1, color = color,width=0.99)\n",
    "    ax1.set_xlabel('time (s)')\n",
    "    ax1.set_ylabel(data1_ylabel, color = color)\n",
    "    ax1.tick_params(axis ='y', labelcolor = color)\n",
    "    ax2 = ax1.twinx()\n",
    "   \n",
    "    color = colors[1]\n",
    "    ax2.set_ylabel(data2_ylabel, color = color)\n",
    "    ax2.bar(timedata, data2, color = color)\n",
    "    ax2.tick_params(axis ='y', labelcolor = color)\n",
    "       \n",
    "    if  plot_vertical_line==True:\n",
    "        for i in range(len(vertical_line_points)):\n",
    "            if i==0:\n",
    "                plt.axvline(x = vertical_line_points[i] , color = 'b', label = 'axvline - full height')\n",
    "            else:\n",
    "                plt.axvline(x = vertical_line_points[i] , color = 'r', label = 'axvline - full height')\n",
    "\n",
    "\n",
    "    fig.suptitle(title, fontweight =\"bold\")\n",
    "    if is_save_plot==True:\n",
    "        save_plot(save_bucket,save_key,save_format)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27ec0295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_plot(bucket,key,save_format):\n",
    "    \"\"\"\n",
    "    Save a plot to a specific bucket, plot must be aloready plotted \n",
    "    Arguments:\n",
    "        bucket: bucket location\n",
    "        key: key\n",
    "        save_format: file format of the saved plot \n",
    "\n",
    "    Returns:\n",
    "       No return value, saves plot to bucket.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(bucket,key)\n",
    "    img_data = io.BytesIO()\n",
    "    plt.savefig(img_data, format=save_format)\n",
    "    img_data.seek(0)\n",
    "    s3 = bt.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    bucket.put_object(Body=img_data, ContentType='image/{}'.format(save_format), Key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e6aab2",
   "metadata": {},
   "source": [
    "# Save Model and Scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc95f5d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_models_info_totable(table_path,train_info_dict,cust_features_dict):\n",
    "    \"\"\"\n",
    "    Read model ifnormation from a table, update the table with the new model training information and customer information, then save the model informations for later search and model retrieval to a parquet file. \n",
    "    Arguments:\n",
    "        table_path: the original table to be updated\n",
    "        train_info_dict: dictionary that hold the information to be inserted in to the table\n",
    "        cust_features_dict: contains customer infomation such as location, asset, subasset\n",
    "\n",
    "    Returns:\n",
    "        Returns the latest addition to the table.\n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    df=pd.read_parquet(table_path)\n",
    "    \n",
    "    columns_notIn_table=[ column for column in train_info_dict[\"features_for_model_info_table\"] if column not in df.columns ]\n",
    "    df[columns_notIn_table]=None\n",
    "    \n",
    "    train_info_dict.update(cust_features_dict)\n",
    "    features_for_model_info_dict={key: train_info_dict[key] for key in train_info_dict[\"features_for_model_info_table\"]}\n",
    "    features_for_model_info_dict.update({\"train_cols\":\"-\".join(train_info_dict[\"train_cols\"])})\n",
    "    new_row_df=pd.DataFrame(features_for_model_info_dict,index=[-1])\n",
    "    df=pd.concat([df,new_row_df])\n",
    "    \n",
    "    df.index = df.index + 1\n",
    "    df = df.sort_index()\n",
    "    \n",
    "    df.to_parquet(table_path)\n",
    "    \n",
    "    return df.iloc[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c9d9c9a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_with_info(train_info_dict):\n",
    "    \"\"\"\n",
    "    Gets a train information dictionary, extracts the file location information etc and saves to the appropriate locations. \n",
    "    This function should be run after the training of a model. \n",
    "    trained columns\n",
    "    trained time intervals\n",
    "    model in h5 format\n",
    "    scaler in gz format\n",
    "    training parameters: window size etc...\n",
    "    \n",
    "    \n",
    "    Arguments:\n",
    "        train_info_dict: contains the training parameters, customer information such as geolocation, model and model information\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       Returns the updated train_info_dict with updated tools_path, customer features dictionary \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    train_info_dict,cust_features_dict=create_model_and_scaler_name(train_info_dict)\n",
    "    \n",
    "    tools_path=train_info_dict[\"tools_path_prefix\"]+cust_features_dict[\"cust_path\"]+train_info_dict[\"model_name\"]+\"/\"\n",
    "    train_info_dict.update({\"tools_path\":tools_path})\n",
    "    \n",
    "    ####\n",
    "    tools_path_s3_bucket_name,tools_path_s3_key,_=get_s3_Bucketname_and_Key(tools_path)\n",
    "    plot_s3_key=tools_path_s3_key+\"loss_history.\"+train_info_dict[\"plot_save_format\"]\n",
    "    plot_twinx(train_info_dict[\"loss_history\"],train_info_dict[\"loss_history\"],range(len(train_info_dict[\"loss_history\"])),\"loss-validation_loss\",\"val_loss_history\",\"loss_history\",is_save_plot=True,save_bucket=tools_path_s3_bucket_name,save_key=plot_s3_key,save_format=train_info_dict[\"plot_save_format\"],plot_vertical_line=False) \n",
    "    \n",
    "    save_model_or_scaler_s3(train_info_dict[\"model\"],train_info_dict[\"tools_path\"],train_info_dict[\"model_name\"])\n",
    "    save_model_or_scaler_s3(train_info_dict[\"scaler\"],train_info_dict[\"tools_path\"],train_info_dict[\"scaler_name\"])\n",
    "    train_info_dict[\"train_data_info\"].to_parquet(train_info_dict[\"tools_path\"]+\"train_data_info.parquet\")\n",
    "    pd.DataFrame(train_info_dict[\"train_cols\"],columns=[\"train_cols\"]).to_parquet(train_info_dict[\"tools_path\"]+\"train_cols.parquet\")\n",
    "    pd.DataFrame([train_info_dict[\"time_column\"]],columns=[\"time_column\"]).to_parquet(train_info_dict[\"tools_path\"]+\"time_column.parquet\")\n",
    "    \n",
    "    return train_info_dict, cust_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a8b8e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_and_scaler_name(train_info_dict):\n",
    "    \"\"\"\n",
    "    Creates a name for the scaler and the model, the model name if formed using the \"model_name_content_inorder\" structure. Later updates the model and scaler names from the training parameters\n",
    "    \n",
    "    Arguments:\n",
    "        train_info_dict: contains the training parameters, customer information such as geolocation, model and model information\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       Returns the updated train_info_dict with updated tools_path, customer features dictionary \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    cust_features_dict=get_cust_features(train_info_dict[\"read_path\"], train_info_dict[\"cust_list\"])\n",
    "    train_info_dict.update(cust_features_dict)\n",
    "    if type(cust_features_dict)==dict:\n",
    "        \n",
    "        train_features_dict={key: train_info_dict[key] for key in train_info_dict[\"model_name_content_inorder\"]}\n",
    "        scaler_name=\"_\".join([str(train_feature) for train_feature in train_features_dict.values() ])+\"_scaler\"+\".gz\"\n",
    "        model_name=\"_\".join([str(train_feature) for train_feature in train_features_dict.values() ])+\"_model\"+\".h5\"\n",
    "        \n",
    "        train_info_dict.update({\"model_name\":model_name,\"scaler_name\":scaler_name})\n",
    "                    \n",
    "        return train_info_dict,cust_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c291ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cust_features(path,cust_list=[\"nurol\",\"kutahya-ser\",\"celik-halat\"]):\n",
    "\n",
    "    \"\"\"\n",
    "    Scans the path untill one of the items in the customer list is found (in the subfolder structure) and creates a customer features dictionary\n",
    "    \n",
    "    Arguments:\n",
    "        path: path\n",
    "        cust_list: list of customer names that we want to detect\n",
    "\n",
    "\n",
    "    Returns:\n",
    "       Returns the updated train_info_dict with updated tools_path, customer features dictionary \n",
    "        \n",
    "    \"\"\"\n",
    "    for cust in cust_list:\n",
    "        name_start_index=path.find(cust)\n",
    "        if name_start_index!=-1:\n",
    "            cust_features_list=path[name_start_index:].split(\"/\")[:5]\n",
    "            cust_features_names=[\"cust\",\"country\",\"location\",\"asset\",\"subasset\"]\n",
    "            cust_features_dict={cust_features_names[i]: cust_features_list[i] for i in range(len(cust_features_list))}\n",
    "            cust_path=\"\".join([feature+\"/\" for feature in cust_features_dict.values() ])\n",
    "            cust_features_dict.update({\"cust_path\":cust_path})\n",
    "            return cust_features_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "094562dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_or_scaler_s3(model_or_scaler,save_path,model_or_scaler_name):\n",
    "    \"\"\"\n",
    "    Save the model or the scaler to an s3 bucket. \n",
    "    Arguments:\n",
    "        model_or_scaler: the model or the scaler object to be saved in h5 or gz format\n",
    "        save_path: location to save the model\n",
    "        model_or_scaler_name: name \n",
    "\n",
    "    Returns:\n",
    "        No return value.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = bt.client('s3')\n",
    "        save_path_s3_bucket_name,save_path_s3_key,_=get_s3_Bucketname_and_Key(save_path)\n",
    "        model_or_scaler_s3_key=save_path_s3_key+model_or_scaler_name\n",
    "        with tempfile.TemporaryFile() as fp:\n",
    "            joblib.dump(model_or_scaler, fp)\n",
    "            fp.seek(0)\n",
    "            s3_client.put_object(Body=fp.read(), Bucket=save_path_s3_bucket_name, Key=model_or_scaler_s3_key)\n",
    "        clear_output()\n",
    "        print(\"Successfully Saved to {}\".format(save_path))\n",
    "    except Exception as E:\n",
    "        print(E)\n",
    "        print(\"Unsuccessfully Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4fb8a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_model_or_scaler_s3(model_or_scaler_read_path,model_or_scaler_name):\n",
    "    \"\"\"\n",
    "    Read the model in h5 format or scaler in gz format from the specified path and model name and return the model or scaler object\n",
    "    Arguments:\n",
    "        model_or_scaler_read_path: path of the model directory\n",
    "        model_or_scaler_name: the name of the model or scaler file in h5 or gz format\n",
    "    Returns:\n",
    "        Returns the model or scaler object.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = bt.client('s3')\n",
    "        read_path_s3_bucket_name,read_path_s3_key,_=get_s3_Bucketname_and_Key(model_or_scaler_read_path)\n",
    "        model_or_scaler_s3_key=read_path_s3_key+model_or_scaler_name\n",
    "        with tempfile.TemporaryFile() as fp:\n",
    "            s3_client.download_fileobj(Fileobj=fp, Bucket=read_path_s3_bucket_name, Key=model_or_scaler_s3_key)\n",
    "            fp.seek(0)\n",
    "            model_or_scaler = joblib.load(fp)\n",
    "        clear_output()\n",
    "        print(\"Successfully Read\")\n",
    "        return model_or_scaler\n",
    "    except: \n",
    "        print(\"Unsuccessfully Read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "1fe51402",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_model_or_scaler_s3(model_or_scaler_read_path_and_name): #(UNTESTED)\n",
    "    \"\"\"\n",
    "    Deletes the specified model\n",
    "    \n",
    "    Arguments:\n",
    "        model_or_scaler_read_path: path of the model directory AND filename\n",
    "    Returns:\n",
    "        Returns nothing.\n",
    "        \n",
    "    \"\"\"\n",
    "    try:\n",
    "        s3_client = bt.client('s3')\n",
    "        s3_bucket_name,s3_key,s3_file_name=get_s3_Bucketname_and_Key(model_or_scaler_read_path_and_name)\n",
    "        s3_client.delete_object(Bucket=s3_bucket_name, Key=s3_key)\n",
    "        print(\"Successfully Deleted\")\n",
    "    except: \n",
    "        print(\"Unsuccessfully Deleted\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102e2b1b",
   "metadata": {},
   "source": [
    "# Train Funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2103ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_generic_on_batches(train_info_dict):    # must be modified if non aut encoder\n",
    "    \"\"\"\n",
    "    Trains an auto encoder model, all model and training info are contained in the train_info_dict dictionary\n",
    "    \n",
    "    Arguments:\n",
    "        train_info_dict: all information of the model and training\n",
    "    Returns:\n",
    "        train_info_dict: updated train info dictionary\n",
    "        loss_history: loss information \n",
    "        val_loss_history: validation loss history\n",
    "        all_loss: empty list, not used\n",
    "    \"\"\"\n",
    "    early_stopping_patience=train_info_dict[\"early_stopping_patience\"]\n",
    "    loss_history=[]\n",
    "    val_loss_history=[]\n",
    "    all_loss=[]\n",
    "    \n",
    "    for epoch in range(train_info_dict[\"n_epochs\"]):\n",
    "        print(epoch)\n",
    "        batch_loss_history=[]\n",
    "        for i in tqdm(range(len(train_info_dict[\"train_data_info\"]))):\n",
    "            train_df=read_spesific_files(train_info_dict[\"read_path\"],start_end_df=train_info_dict[\"train_data_info\"].iloc[i:i+1],use_cols=train_info_dict[\"use_cols\"],time_column=train_info_dict[\"time_column\"])\n",
    "            if len(train_df)==0:\n",
    "                continue\n",
    "                \n",
    "            if train_info_dict[\"rolling_window_time\"]!=None:\n",
    "                train_df.index=train_df[train_info_dict[\"time_column\"]]\n",
    "                train_cols_func_dict={column:[train_info_dict[\"rolling_window_func\"]] for column in train_info_dict[\"train_cols\"]}                \n",
    "                train_df.loc[:,train_info_dict[\"train_cols\"]]=train_df[train_info_dict[\"train_cols\"]].rolling(train_info_dict[\"rolling_window_time\"]).agg(train_cols_func_dict)\n",
    "                train_df.columns=train_df.columns.get_level_values(0)\n",
    "                train_df.reset_index(drop=True,inplace=True)\n",
    "            \n",
    "            train_df_segment_step=int(len(train_df)/train_info_dict[\"train_df_segment_num\"])\n",
    "                                      \n",
    "            for df_segment_order in range(train_info_dict[\"train_df_segment_num\"]):\n",
    "                X_normal,batch_num=create_segment_data(train_df,df_segment_order,train_df_segment_step,train_info_dict)\n",
    "                if train_info_dict[\"shuffle\"]==True:\n",
    "                    np.random.shuffle(X_normal)\n",
    "                for batch_order in range(batch_num):\n",
    "                    train_info_dict,batch_loss=train_model_on_batch(X_normal,batch_order,train_info_dict) #X_normal = x normal data CHANGE IF NOT AN AUTOENCODER\n",
    "                    batch_loss_history.append(batch_loss)\n",
    "    \n",
    "        loss_history.append(mean(batch_loss_history))\n",
    "        \n",
    "        #val_loss= model.evaluate(X_valid, expit(X_valid))\n",
    "        #val_loss_history.append(val_loss)\n",
    "        clear_output(wait=True)\n",
    "        plot_twinx(loss_history,loss_history,range(len(loss_history)),\"loss-validation_loss\",\"val_loss_history\",\"loss_history\",is_save_plot=False,plot_vertical_line=False)    \n",
    "        \n",
    "        if epoch!=0:\n",
    "            if loss_history[epoch]>loss_history[epoch-1]:\n",
    "                early_stopping_patience-=1\n",
    "            if early_stopping_patience==0:\n",
    "                break\n",
    "    return train_info_dict,loss_history,val_loss_history,all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "98b8dc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_on_batch(X_toTrain,batch_order,train_info_dict): # UPDATE FOR NON AUTOENCODERS\n",
    "    \"\"\"\n",
    "    Trains the model on  batch\n",
    "    \n",
    "    Arguments:\n",
    "        X_toTrain: array to train the model\n",
    "        batch_order: if the xarray data is 1024 items long, which windows are we training on.\n",
    "        train_info_dict: all model and training info\n",
    "    Returns:\n",
    "        train_info_dict: updated train info dictionary\n",
    "        batch_loss: loss for this batch\n",
    "    \"\"\"    \n",
    "    train_batch=X_toTrain[batch_order*train_info_dict[\"batch_size\"]:(batch_order+1)*train_info_dict[\"batch_size\"]]\n",
    "    \n",
    "    if train_info_dict[\"model_input_shape_len\"]==2:\n",
    "        X_toTrain=X_toTrain.reshape(X_toTrain.shape[0],X_toTrain.shape[1])\n",
    "    \n",
    "    batch_loss= train_info_dict[\"model\"].train_on_batch(train_batch,train_batch)\n",
    "\n",
    "    return train_info_dict,batch_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368cb025",
   "metadata": {},
   "source": [
    "# PREPARE DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3e0e2e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(df,train_info_dict):\n",
    "    if train_info_dict[\"sample_time\"]!=None:\n",
    "        numeric_cols = list(df.select_dtypes(include=np.number).columns)\n",
    "        object_cols = list(df.select_dtypes(include=np.object_).columns)\n",
    "        numeric_cols_func_dict={column:[train_info_dict[\"sample_time_func\"]] for column in numeric_cols}\n",
    "        object_cols_fync_dict={column:[\"sum\"] for column in object_cols}\n",
    "        cols_func_dict={**numeric_cols_func_dict,**object_cols_fync_dict}\n",
    "        \n",
    "        df.set_index(train_info_dict[\"time_column\"],inplace=True)\n",
    "        df_sampled=df.resample(train_info_dict[\"sample_time\"]).agg(cols_func_dict)\n",
    "        df_sampled.columns=df_sampled.columns.get_level_values(0)\n",
    "        df.reset_index(inplace=True)\n",
    "        df_sampled.reset_index(inplace=True)\n",
    "        df_sampled.dropna(subset=train_info_dict[\"train_cols\"],inplace=True)\n",
    "    \n",
    "    elif train_info_dict[\"sample_time\"]==None:\n",
    "        df_sampled=df.copy(deep=True)\n",
    "        df_sampled.dropna(subset=train_info_dict[\"train_cols\"],inplace=True)\n",
    "    \n",
    "    try:\n",
    "        df_sampled[train_info_dict[\"train_cols\"]]=train_info_dict[\"scaler\"].transform(df_sampled[train_info_dict[\"train_cols\"]])\n",
    "        \n",
    "    except:\n",
    "        print(df_sampled)\n",
    "        print(\"we cant scale data\")\n",
    "    \n",
    "    \n",
    "    X_,X_list,sequence_df_list,divide_idx_list=get_divided_sequences(df_sampled,train_info_dict)\n",
    "    return X_,X_list,sequence_df_list,divide_idx_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88315cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_divided_sequences(df_sampled,train_info_dict):\n",
    "    divide_idx_list=find_divide_idx(df_sampled,window_size=train_info_dict[\"window_size\"],sample_time=train_info_dict[\"sample_time\"],time_column=train_info_dict[\"time_column\"])\n",
    "    divide_idx_list=[-1]+divide_idx_list+[99999999999]\n",
    "    print(\"divided points idxs\")\n",
    "    print(divide_idx_list[1:-1])\n",
    "    \n",
    "    X_list=[]\n",
    "    sequence_df_list=[]\n",
    "    for i in range(len(divide_idx_list)-1):\n",
    "        sequence=df_sampled.iloc[divide_idx_list[i]+1:divide_idx_list[i+1]+1]\n",
    "        X_= rolling_window(sequence[train_info_dict[\"train_cols\"]], window_size=train_info_dict[\"window_size\"],features_num=len(train_info_dict[\"train_cols\"]))\n",
    "        if len(X_)>0:\n",
    "            X_list.append(X_)\n",
    "            sequence_df_list.append(sequence)\n",
    "    \n",
    "    if len(X_list)>0:\n",
    "        X_=np.concatenate(X_list)\n",
    "    else:\n",
    "        X_=np.array([]).reshape((0,train_info_dict[\"window_size\"],len(train_info_dict[\"train_cols\"]))) # to have empty array with size which is equal to model input size \n",
    "    \n",
    "    \n",
    "    return X_,X_list,sequence_df_list,divide_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e876bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(array, window_size,features_num ,stride=1):\n",
    "    if len(array)>=(window_size):\n",
    "        array = np.array(array)\n",
    "        shape = (array.shape[0] - window_size + 1, window_size,features_num)\n",
    "        strides = (array.strides[0],) + array.strides\n",
    "        rolled = np.lib.stride_tricks.as_strided(array, shape=shape, strides=strides)\n",
    "        return rolled[np.arange(0, shape[0], stride)]\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3eb252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences_X(x, window_size=128):###########################################\n",
    "    \n",
    "    x_values = []\n",
    "    for i in range(len(x)-(window_size-1)):\n",
    "        x_values.append(x.iloc[i:(i+window_size)].values)\n",
    "        \n",
    "    return np.array(x_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d74bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_divide_idx(df,window_size,sample_time,time_column):###########################################\n",
    "\n",
    "    divide_points_idx_list=[]\n",
    "    \n",
    "    if sample_time!=None:\n",
    "        window_time=pd.Timedelta(sample_time)*window_size\n",
    "    elif sample_time==None:\n",
    "        window_time=((df[time_column].iloc[-1]-df[time_column].iloc[0])/len(df))*window_size #this row may cause problems\n",
    "\n",
    "    \n",
    "    print(\"average time diff between rows \"+str(window_time/window_size))\n",
    "    \n",
    "    time_serie=np.array(df[time_column])\n",
    "    \n",
    "    for i in range(len(df)-1):    \n",
    "        \n",
    "        before_time=time_serie[i]\n",
    "        after_time=time_serie[i+1]    \n",
    "        \n",
    "        if ((after_time-before_time)>window_time):\n",
    "            divide_points_idx_list.append(i)\n",
    "            \n",
    "            \n",
    "    return divide_points_idx_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ba3c116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_segment_data(train_df,df_segment_order,train_df_segment_step,train_info_dict):\n",
    "    train_df_segment=train_df.iloc[df_segment_order*train_df_segment_step:(df_segment_order+1)*train_df_segment_step]\n",
    "    X_,X_list,sequence_df_list,divide_idx_list=get_data(train_df_segment,train_info_dict)\n",
    "    X_=np.asarray(X_).astype('float32')\n",
    "    batch_num=int(np.ceil((len(X_)/train_info_dict[\"batch_size\"])))\n",
    "    return X_,batch_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53a1dc8",
   "metadata": {},
   "source": [
    "# TEST FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a1624963",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_predict_errors(X,reconstructed_X,cols,loss_type):###########################################\n",
    "    df=pd.DataFrame([])\n",
    "    for i,col in enumerate(cols):\n",
    "        try:\n",
    "            if loss_type==\"mse\":\n",
    "                if len(X.shape)==2:\n",
    "                    predicted_error=tf.keras.losses.mse(reconstructed_X,X)\n",
    "                else :\n",
    "                    predicted_error=tf.keras.losses.mse(reconstructed_X[:,:,i],X[:,:,i])\n",
    "            elif loss_type==\"mae\":\n",
    "                if len(X.shape)==2:\n",
    "                    predicted_error=tf.keras.losses.mae(reconstructed_X,X)\n",
    "                else :\n",
    "                    predicted_error=tf.keras.losses.mae(reconstructed_X[:,:,i],X[:,:,i])\n",
    "            df[\"{}_ae_{}\".format(col,loss_type)]=predicted_error\n",
    "        except Exception as exception:\n",
    "            print(exception)\n",
    "            print(\"you may use loss_type except mae or mse\")\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "275f00aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monolithic_anomaly_df(X_list,sequence_df_list,train_info_dict):\n",
    "    anomaly_dfs=[]\n",
    "    for j in range(len(X_list)):\n",
    "        anomaly_df=create_anomaly_df(X_=X_list[j],sequence_df=sequence_df_list[j],train_info_dict=train_info_dict)\n",
    "        anomaly_dfs.append(anomaly_df)\n",
    "    if len(anomaly_dfs)>0:\n",
    "        anomaly_df=pd.concat(anomaly_dfs)\n",
    "        return anomaly_df\n",
    "    else:\n",
    "        return pd.DataFrame([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a2fe31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sequenced_columns(X_,cols,window_size):###########################################\n",
    "    df=pd.DataFrame([])\n",
    "    for i,col in enumerate(cols):\n",
    "        if len(X_.shape)==3:\n",
    "            lineared_column =reconstructed_array_to_signal_mean(change_shape(X_[:,:,i:i+1]),window_size)\n",
    "            df[col]=lineared_column\n",
    "        else:\n",
    "            lineared_column =reconstructed_array_to_signal_mean(X_,window_size)\n",
    "            df[col]=lineared_column\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd8b57a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomaly_df(X_,sequence_df,train_info_dict):\n",
    "\n",
    "    anomaly_df=pd.DataFrame(sequence_df[:(-train_info_dict[\"window_size\"]+1)][train_info_dict[\"time_column\"]])\n",
    "    if train_info_dict[\"model_input_shape_len\"]==2:\n",
    "        X_=X_.reshape(X_.shape[0],X_.shape[1])\n",
    "    reconstructions_=train_info_dict[\"model\"].predict(X_)\n",
    "    \n",
    "    predicted_errors_df=get_model_predict_errors(X_,reconstructions_,train_info_dict[\"train_cols\"],train_info_dict[\"loss_type\"])  \n",
    "\n",
    "    lineared_reconstructed_columns=linear_sequenced_columns(reconstructions_,train_info_dict[\"train_cols\"],train_info_dict[\"window_size\"])\n",
    "\n",
    "    predicted_error_column_names=[train_info_dict[\"error_column_template\"].format(col,train_info_dict[\"loss_type\"]) for col in train_info_dict[\"train_cols\"]]\n",
    "    anomaly_df.reset_index(inplace=True,drop=True)\n",
    "    anomaly_df[predicted_error_column_names]=predicted_errors_df\n",
    "\n",
    "    anomaly_df=pd.merge(left=sequence_df,right=anomaly_df,how=\"outer\",on=train_info_dict[\"time_column\"])\n",
    "    anomaly_df[[train_info_dict[\"reconstructed_column_template\"].format(col) for col in train_info_dict[\"train_cols\"]]]=lineared_reconstructed_columns\n",
    "    \n",
    "    \n",
    "    return anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ca9027b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_info_dict(one_model_info_table):\n",
    "    train_info_dict=one_model_info_table.to_dict('records')[0]\n",
    "    \n",
    "    model=read_model_or_scaler_s3(train_info_dict[\"tools_path\"],train_info_dict[\"model_name\"])\n",
    "    scaler=read_model_or_scaler_s3(train_info_dict[\"tools_path\"],train_info_dict[\"scaler_name\"])\n",
    "    train_cols=pd.read_parquet(train_info_dict[\"tools_path\"]+\"train_cols.parquet\").values.ravel()\n",
    "    time_column=pd.read_parquet(train_info_dict[\"tools_path\"]+\"time_column.parquet\").values.ravel()[0]\n",
    "    use_cols=list(train_cols)+[time_column]\n",
    "    train_data_info=pd.read_parquet(train_info_dict[\"tools_path\"]+\"train_data_info.parquet\")\n",
    "    \n",
    "    train_info_dict.update({\n",
    "    \"model\":model,\n",
    "    \"scaler\":scaler,\n",
    "    \"train_cols\":list(train_cols),\n",
    "    \"time_column\":time_column,\n",
    "    \"use_cols\":use_cols,\n",
    "    \"train_data_info\":train_data_info\n",
    "    })\n",
    "    \n",
    "    return train_info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3049bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_cust_subassets_models_info(read_path,model_info_with_wanted_features,cust_features_column_names, cust_list):\n",
    "    cust_features_dict=get_cust_features(read_path, cust_list=cust_list)\n",
    "    cust_subasset_mask=[(model_info_with_wanted_features[feature_key]==feature_value) for feature_key,feature_value in zip(cust_features_column_names,cust_features_dict.values())]\n",
    "    cust_subasset_models_info=model_info_with_wanted_features.loc[sum(cust_subasset_mask)==len(cust_features_column_names)]\n",
    "    \n",
    "    return cust_subasset_models_info,cust_features_dict[\"cust_path\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21af339c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8936fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb808c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78ec30f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "6cc27e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_anomaly_df(X_,sequence_df,train_info_dict):\n",
    "\n",
    "    anomaly_df=pd.DataFrame(sequence_df[:(-train_info_dict[\"window_size\"]+1)][train_info_dict[\"time_column\"]])\n",
    "    if train_info_dict[\"model_input_shape_len\"]==2:\n",
    "        X_=X_.reshape(X_.shape[0],X_.shape[1])\n",
    "    reconstructions_=train_info_dict[\"model\"].predict(X_)\n",
    "\n",
    "    predicted_errors_df=get_model_predict_errors(X_,reconstructions_,train_info_dict[\"train_cols\"],train_info_dict[\"loss_type\"])  \n",
    "\n",
    "    lineared_reconstructed_columns=linear_sequenced_columns(reconstructions_,train_info_dict[\"train_cols\"],train_info_dict[\"window_size\"],train_info_dict[\"reconstructed_column_template\"])\n",
    "\n",
    "    predicted_error_column_names=[train_info_dict[\"error_column_template\"].format(col,train_info_dict[\"loss_type\"]) for col in train_info_dict[\"train_cols\"]]\n",
    "    anomaly_df.reset_index(inplace=True,drop=True)\n",
    "    anomaly_df[predicted_error_column_names]=predicted_errors_df\n",
    "\n",
    "    anomaly_df=pd.merge(left=sequence_df,right=anomaly_df,how=\"outer\",on=train_info_dict[\"time_column\"])\n",
    "    anomaly_df=pd.concat([anomaly_df,lineared_reconstructed_columns],axis=1)    \n",
    "\n",
    "    return anomaly_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "71cc9f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_sequenced_columns(X_,cols,window_size,reconstructed_column_template):###########################################\n",
    "    df=pd.DataFrame([])\n",
    "    for i,col in enumerate(cols):\n",
    "        if len(X_.shape)==3:\n",
    "            reconstructed_array_signal_mean =reconstructed_array_to_signal_mean(change_shape(X_[:,:,i:i+1]),window_size)\n",
    "            reconstructed_array_signal_variance =reconstructed_array_to_signal_variance(change_shape(X_[:,:,i:i+1]),window_size)\n",
    "            df[reconstructed_column_template.format(col)+\"_mean\"]=reconstructed_array_signal_mean\n",
    "            df[reconstructed_column_template.format(col)+\"_variance\"]=reconstructed_array_signal_variance\n",
    "\n",
    "        else:\n",
    "            reconstructed_array_signal_mean =reconstructed_array_to_signal_mean(X_,window_size)\n",
    "            reconstructed_array_signal_variance =reconstructed_array_to_signal_variance(X_,window_size)\n",
    "\n",
    "            df[reconstructed_column_template.format(col)+\"_mean\"]=reconstructed_array_signal_mean\n",
    "            df[reconstructed_column_template.format(col)+\"_variance\"]=reconstructed_array_signal_variance\n",
    "    \n",
    "    return df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
