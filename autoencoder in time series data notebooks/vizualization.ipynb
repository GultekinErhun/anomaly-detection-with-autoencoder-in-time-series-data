{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87320fb",
   "metadata": {},
   "source": [
    "# IMPORT LIB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ed3dd0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'import_ipynb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mimport_ipynb\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mAE_LIB\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      4\u001b[0m pd\u001b[38;5;241m.\u001b[39mset_option(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisplay.max_columns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'import_ipynb'"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from AE_LIB import *\n",
    "\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.style as mplstyle  # ask Kudret for optimizations\n",
    "mplstyle.use('fast')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58383760",
   "metadata": {},
   "source": [
    "# EXTRA FUNCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8174c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_aggregated_df(anomaly_df,time_column,error_column_names,statiscital_func_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Returns a dataframe of values containing an output for each of the statistical functions in the statistical function list , \n",
    "    Arguments:\n",
    "        anomaly_df: Dataframe that contains the info for one cycle\n",
    "        time_column: which column of the parquet file will be transformed to a date time and for sorting\n",
    "        error_column_names: List of columns for which the statistical functions should be applied \n",
    "        statiscital_func_list: list of statistical functions ex:\"sum\",\"mean\",\"median\",\"std\",\"min\",\"max\",\"var\",\"sem\",\"skew\",\"kurt\".\n",
    "        These functions are defined in the scikitlearn functions. \n",
    "        Custom functions can also be written and the name has to be passed to it.\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        Returns a fresh dataframe with the select column names and selected functions. New columns are named  \n",
    "    \"\"\"\n",
    "    aggregated_df=(anomaly_df[error_column_names].dropna(axis=0, how='any',inplace=False)).aggregate(statiscital_func_list)\n",
    "    aggregated_df.loc[\"max_min_diff\"]=aggregated_df.loc[\"max\"]-aggregated_df.loc[\"min\"] # could also have been defined as s separate function\n",
    "    aggregated_df[\"cycle_num\"]=anomaly_df[\"cycle_num\"].iloc[0]\n",
    "    aggregated_df[\"start\"]= anomaly_df[time_column].iloc[0]\n",
    "    aggregated_df[\"end\"]= anomaly_df[time_column].iloc[-1]\n",
    "    return aggregated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc053a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_figure(bucket,key,save_format):\n",
    "    \"\"\"\n",
    "    Save figures to buckets using keys and a file format, \n",
    "    Arguments:\n",
    "        bucket: Name of S3 bucket\n",
    "        key: key in S3\n",
    "        save_format: File type of the saved figure. i.e pdf...\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "       Does not have a return value, saves figure in the specified place\n",
    "    \"\"\"\n",
    "    \n",
    "    print(bucket,key)\n",
    "    img_data = io.BytesIO()\n",
    "    fig.savefig( img_data, format=save_format)\n",
    "    img_data.seek(0)\n",
    "    s3 = bt.resource('s3')\n",
    "    bucket = s3.Bucket(bucket)\n",
    "    bucket.put_object(Body=img_data, ContentType='image/{}'.format(save_format), Key=key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bd0d09",
   "metadata": {},
   "source": [
    "# 1) READ MODEL INFORMATION TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4060a2f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_model_info=pd.read_parquet(\"all_models_info_table_v2.parquet\")\n",
    "all_model_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c45252",
   "metadata": {},
   "source": [
    "# 2) CHOOSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5f8476",
   "metadata": {},
   "source": [
    "### 2.1) CHOOSE ONE ROW FROM MODEL INFO TABLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be69f33a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wanted_features_dictt={}\n",
    "dictt={\n",
    "    \"cust\":\"nurol\",\n",
    "    \"country\":None,\n",
    "    \"location\":None,\n",
    "    \"asset\":None,\n",
    "    \"subasset\":None,\n",
    "    \"device_type\":None,\n",
    "    \"ID\":None,\n",
    "    \"window_size\":128,\n",
    "    \"sample_time\":None,\n",
    "    \"rolling_window_time\":None,\n",
    "    \"bottleneck\":None,\n",
    "    \"wideneck\":None,\n",
    "    \"scaler_type\":None,\n",
    "    \"layers_type\":None,\n",
    "    \"column_num\":None,\n",
    "    \"loss_type\":None,\n",
    "    \"epoch_num\":None,\n",
    "    \"total_process_num\":None,\n",
    "    \"date_of_save\":None,\n",
    "    \"train_data_note\":None,\n",
    "    \"tools_path\":None,\n",
    "    \"model_name\":None,\n",
    "    \"scaler_name\":None,\n",
    "    \"is_live\":None,\n",
    "    \"author\":None\n",
    "    }\n",
    "for a,b in dictt.items():\n",
    "    if b!=None:\n",
    "        print(a,b)\n",
    "        wanted_features_dictt.update({a:b})\n",
    "\n",
    "if  len(wanted_features_dictt)>0:\n",
    "    mask=[(all_model_info[feature_key]==feature_value) for feature_key,feature_value in wanted_features_dictt.items()]\n",
    "    model_info_with_wanted_features=all_model_info.loc[sum(mask)==len(wanted_features_dictt.items())]\n",
    "    \n",
    "model_info_with_wanted_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c41579",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_model_info_df=model_info_with_wanted_features.iloc[0:1]\n",
    "one_model_info_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd5625c",
   "metadata": {},
   "source": [
    "# 3) GET OPTIONS OF CHOOSEN MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a61a329",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Convert df to dictionary\n",
    "# Insert additional features such as model\n",
    "train_info_dict=get_train_info_dict(one_model_info_df)\n",
    "\n",
    "\n",
    "# get the subasset path from train info dict\n",
    "cust_features_dict=get_cust_features(train_info_dict[\"read_path\"])\n",
    "\n",
    "\n",
    "#extract error column names list\n",
    "error_column_names=[train_info_dict[\"error_column_template\"].format(col,train_info_dict[\"loss_type\"]) for col in train_info_dict[\"train_cols\"]]\n",
    "# generate the reconstructed colum names list\n",
    "reconstructed_column_names=[train_info_dict[\"reconstructed_column_template\"].format(col) for col in train_info_dict[\"train_cols\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f062df84",
   "metadata": {},
   "source": [
    "# 4) PARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdb8ed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read_path_prefix = \"s3://test-data-eng/anomaly_detection/autoencoder/outputs/\"\n",
    "\n",
    "# WHERE WE WILL READ THE DATA\n",
    "read_path_prefix = \"s3://test-data-eng/anomaly_detection/autoencoder/outputs_teoman/\"\n",
    "\n",
    "#customer\n",
    "cust_features_column_names=[\"cust\",\"country\",\"location\",\"asset\",\"subasset\"]# \"device_type\",\"ID\" are canceled\n",
    "\n",
    "# statistical functions\n",
    "statiscital_func_list=[\"sum\",\"mean\",\"median\",\"std\",\"min\",\"max\",\"var\",\"sem\",\"skew\",\"kurt\"]\n",
    "\n",
    "add_aggregated_df_to_list=True  # get aggregated summary for each cycle\n",
    "add_anomaly_df_to_list=False    # do we want the aggregated anomaly values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3942c4f9",
   "metadata": {},
   "source": [
    "# 5) CREATE STATISTICAL DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9caf4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "anomaly_dfs=[]\n",
    "aggregated_dfs=[]\n",
    "\n",
    "\n",
    "read_path=read_path_prefix+cust_features_dict[\"cust_path\"]+train_info_dict[\"model_name\"]\n",
    "file_paths = wr.s3.list_objects(read_path)\n",
    "\n",
    "# Concate each of the parquet files for a single cycle, i.e. one shift, one day, one month, one operation cycle\n",
    "for file_path in file_paths:\n",
    "    anomaly_df=pd.read_parquet(file_path)\n",
    "    try:\n",
    "        anomaly_df.sort_values(train_info_dict[\"time_column\"],inplace=True)\n",
    "        anomaly_df.reset_index(inplace=True,drop=True)            \n",
    "        aggregated_df=get_aggregated_df(anomaly_df,train_info_dict[\"time_column\"],error_column_names,statiscital_func_list)\n",
    "        \n",
    "        if add_aggregated_df_to_list==True:\n",
    "            aggregated_dfs.append(aggregated_df)\n",
    "        if add_anomaly_df_to_list==True:\n",
    "            anomaly_dfs.append(anomaly_df)\n",
    "    except:\n",
    "        None\n",
    "        \n",
    "    # concatenate the multiple dataframes\n",
    "if add_aggregated_df_to_list==True:\n",
    "    aggregated_df=pd.concat(aggregated_dfs)\n",
    "if add_anomaly_df_to_list==True:\n",
    "    anomaly_df=pd.concat(anomaly_dfs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78c318e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles_statistics=pd.concat(aggregated_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f99001",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles_statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020f1ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles_statistics=cycles_statistics.drop_duplicates() # absurd data may generate multiple of the same rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217a5c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycles_statistics.sort_values(\"start\",inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b19f67c",
   "metadata": {},
   "source": [
    "cycles_statistics=cycles_statistics.iloc[528:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "754d05e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "statiscital_func_list=[\"max_min_diff\"]+statiscital_func_list # could also have been defined as a separate function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e3fd08",
   "metadata": {},
   "source": [
    "# 6) READ MAINTENANCE DF AND RECIPE DF (OPTIONAL-NUROL SPECIFIC )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6bad1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "maintenance_info=pd.read_parquet(\"s3://test-data-eng/anomaly_detection/autoencoder/sources/preprocessed/nurol/tr/golbasi/fct2/fct2_maintenance_infos.parquet\")\n",
    "maintenance_info=maintenance_info.loc[maintenance_info[\"Giriş tarihi\"]<=maintenance_info[\"Fiili btş.trm.\"]]\n",
    "maintenance_info=maintenance_info.groupby(\"Fiili btş.trm.\").aggregate({\"Kısa metin\":\"min\"})\n",
    "maintenance_info.reset_index(inplace=True)\n",
    "maintenance_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0dae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#project name and recipe used\n",
    "used_recipesand_projects=pd.read_parquet(\"s3://test-data-eng/anomaly_detection/autoencoder/sources/preprocessed/nurol/tr/golbasi/fct2/fct2-used_recipes.parquet\")\n",
    "used_recipesand_projects[\"used_recipe\"].dropna(inplace=True)\n",
    "transitions_recipes=[]\n",
    "\n",
    "tmp=None\n",
    "for i in range(len(used_recipesand_projects)):\n",
    "    row_recipe=used_recipesand_projects.iloc[i][\"used_recipe\"]\n",
    "    if tmp!=row_recipe:\n",
    "        transitions_recipes.append(used_recipesand_projects.iloc[i:i+1])\n",
    "    tmp=used_recipesand_projects.iloc[i][\"used_recipe\"]\n",
    "\n",
    "transitions_projects=[]\n",
    "tmp=None    \n",
    "for i in range(len(used_recipesand_projects)):\n",
    "    row_project=used_recipesand_projects.iloc[i][\"project\"]\n",
    "    if tmp!=row_project:\n",
    "        transitions_projects.append(used_recipesand_projects.iloc[i:i+1])\n",
    "    tmp=used_recipesand_projects.iloc[i][\"project\"]    \n",
    "    \n",
    "transitions_recipes =pd.concat(transitions_recipes)\n",
    "transitions_projects =pd.concat(transitions_projects)\n",
    "transitions_projects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5052ad4",
   "metadata": {},
   "source": [
    "# 7) PRINT PLOTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99e3b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_save_format=\"pdf\"\n",
    "plot_save_bucket=\"test-data-eng\"\n",
    "#plot_save_prefix=\"anomaly_detection/autoencoder/outputs_vizualization/concat_1min_kia_multi_column_model_all_subassets.pdf\"\n",
    "plot_save_path_prefix=\"anomaly_detection/autoencoder/vizualization_plots/\"+cust_features_dict[\"cust_path\"]+train_info_dict[\"model_name\"]+\"/\"\n",
    "plot_name= time.strftime(\"%Y-%m-%d %H:%M:%S\", time.localtime())\n",
    "\n",
    "\n",
    "\n",
    "figsize_x=20\n",
    "figsize_y=12\n",
    "plot_quantile_threshold=0.9\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (figsize_x,figsize_y)\n",
    "fig, ax = plt.subplots(len(error_column_names),len(statiscital_func_list),figsize=(figsize_x*len(statiscital_func_list), figsize_y*len(error_column_names)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71817ae0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2c0897",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create a plot in the local folder and save to s3\n",
    "\n",
    "colors =plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "if len(ax.shape)==1: \n",
    "    ax=ax.reshape(1,ax.shape[0]) #if we works with just single column we need this\n",
    "\n",
    "for i,statistical_func in enumerate(statiscital_func_list):\n",
    "    for j,col in enumerate(error_column_names):\n",
    "        maes=cycles_statistics[cycles_statistics.index==statistical_func][error_column_names[j]]\n",
    "        #plot_twinx(maes,maes,cycles_statistics.start.unique(),train_cols[j]+\"--\"+statistical_func,False,bucket=\"data-science-source\",key=\"Anomaly_detection/Autoencoder/png files/{}/{}\".format(statiscital_func_list,train_cols[j]),plot_vertical_line=True,vertical_line_points_df=z[date_columns])\n",
    "        ax[j,i].bar(cycles_statistics.start.unique(),maes, color = colors[1])\n",
    "        ax[j,i].plot(cycles_statistics.start.unique(),maes, color = colors[0])\n",
    "        ax[j,i].set_ylim(maes.min(),maes.quantile(plot_quantile_threshold))\n",
    "        \n",
    "        try:\n",
    "            for k in range(len(maintenance_info)):\n",
    "                if k==0:\n",
    "                    ax[j,i].axvline(x = maintenance_info[\"Fiili btş.trm.\"].iloc[k],label=\"maintenance finish date\", color = colors[2],alpha=0.2)\n",
    "                else:\n",
    "                    ax[j,i].axvline(x = maintenance_info[\"Fiili btş.trm.\"].iloc[k], color = colors[2],alpha=0.2)\n",
    "        except:\n",
    "            None\n",
    "        try:\n",
    "            \n",
    "            for k in range(len(transitions_recipes)):\n",
    "                if k==0:\n",
    "                    ax[j,i].axvline(x = transitions_recipes[\"start\"].iloc[k], label=\"change date of recipe\",color = colors[3],alpha=0.2) \n",
    "                else:\n",
    "                    ax[j,i].axvline(x = transitions_recipes[\"start\"].iloc[k], color = colors[3],alpha=0.2)\n",
    "        except:\n",
    "            None\n",
    "        for k in range(len(transitions_projects)):\n",
    "            if k==0:\n",
    "                ax[j,i].axvline(x = transitions_projects[\"start\"].iloc[k], label=\"change date of project\", color = colors[6],alpha=0.2)\n",
    "            else:\n",
    "                ax[j,i].axvline(x = transitions_projects[\"start\"].iloc[k],  color = colors[6],alpha=0.2)\n",
    "    \n",
    "        ax[j,i].axvline(x = pd.to_datetime(\"2020-11-08\"),  color = \"red\",alpha=0.8,label=\"2020-11-08\")\n",
    "        ax[j,i].axvline(x = pd.to_datetime(\"2023-02-22\"),  color = \"red\",alpha=0.8,label=\"2023-02-22\")\n",
    "        \n",
    "        \n",
    "        ax[j,i].set_title(error_column_names[j]+\"--\"+statistical_func)\n",
    "        ax[j,i].set_xlabel('time')\n",
    "        ax[j,i].set_ylabel('mse_error')\n",
    "        ax[j,i].legend()\n",
    "        \n",
    "fig.savefig( train_info_dict[\"model_name\"]+\"_\"+plot_name+\".\"+plot_save_format)\n",
    "save_figure(plot_save_bucket,plot_save_path_prefix+plot_name+\".\"+plot_save_format,plot_save_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af07217e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
